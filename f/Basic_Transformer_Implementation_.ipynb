{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**What this code is about?**\n",
        "\n",
        "\n",
        "\n",
        "The provided code represents an implementation of the Transformer encoder, a pivotal component of the Transformer architecture used in natural language processing tasks. It includes a MultiHeadAttention module for self-attention mechanisms, a TransformerBlock defining a single processing unit with attention, normalization, skip connections, and a feedforward network, and an Encoder module stacking multiple TransformerBlocks to process input sequences. Although it forms the core of the Transformer, the complete model involves additional components such as the decoder, positional encoding, masking, and other architectural details. The Transformer architecture is widely utilized in various sequence-to-sequence tasks, showcasing its effectiveness in machine translation and language understanding applications."
      ],
      "metadata": {
        "id": "fLqsmxPVWx_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embed_size, heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.heads = heads\n",
        "        self.head_dim = embed_size // heads\n",
        "\n",
        "        assert (\n",
        "            self.head_dim * heads == embed_size\n",
        "        ), \"Embedding size needs to be divisible by heads\"\n",
        "\n",
        "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
        "\n",
        "    def forward(self, values, keys, query, mask):\n",
        "        # Get number of training examples\n",
        "        N = query.shape[0]\n",
        "\n",
        "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
        "\n",
        "        # Split the embedding into self.heads different pieces\n",
        "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
        "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
        "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
        "\n",
        "        values = self.values(values)\n",
        "        keys = self.keys(keys)\n",
        "        queries = self.queries(queries)\n",
        "\n",
        "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
        "\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "\n",
        "        attention = torch.nn.functional.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n",
        "\n",
        "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
        "            N, query_len, self.heads * self.head_dim\n",
        "        )\n",
        "\n",
        "        out = self.fc_out(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "O2khzjjnVCrb"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "    embed_size: The size of the input embeddings.\n",
        "    heads: The number of attention heads. More heads allow the model to focus on different parts of the input sequence simultaneously.\n",
        "    The three linear layers (values, keys, and queries): These layers are used to project the input into different subspaces for each attention head.\n",
        "    fc_out: This linear layer combines the outputs from different attention heads."
      ],
      "metadata": {
        "id": "MIIHhIK6R59y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_size, heads):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention = MultiHeadAttention(embed_size, heads)\n",
        "        self.norm1 = nn.LayerNorm(embed_size)\n",
        "        self.norm2 = nn.LayerNorm(embed_size)\n",
        "\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_size, 4 * embed_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * embed_size, embed_size),\n",
        "        )\n",
        "\n",
        "    def forward(self, value, key, query, mask):\n",
        "        attention = self.attention(value, key, query, mask)\n",
        "\n",
        "        # Add skip connection, run through normalization and finally a feed forward network\n",
        "        x = self.norm1(attention + query)\n",
        "        forward = self.feed_forward(x)\n",
        "        out = self.norm2(forward + x)\n",
        "        return out"
      ],
      "metadata": {
        "id": "jbx2PXdoVEkg"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "    embed_size, heads, dropout, forward_expansion: Parameters for the attention mechanism and feed-forward layers.\n",
        "    attention: Multi-Head Attention module.\n",
        "    norm1 and norm2: Layer normalization is applied before and after the attention and feed-forward layers.\n",
        "    feed_forward: A feed-forward neural network applied to the output of the attention layer.\n",
        "    dropout: Dropout is applied for regularization."
      ],
      "metadata": {
        "id": "UrW8WSIRR-NB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, src_vocab_size, embed_size, num_heads, num_layers):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(src_vocab_size, embed_size)\n",
        "        self.transformer_blocks = nn.ModuleList(\n",
        "            [TransformerBlock(embed_size, num_heads) for _ in range(num_layers)]\n",
        "        )\n",
        "\n",
        "    def forward(self, src, mask):\n",
        "        x = self.embedding(src)\n",
        "\n",
        "        for transformer in self.transformer_blocks:\n",
        "            x = transformer(x, x, x, mask)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "BtDLrLI2VLYN"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "    src_vocab_size, embed_size, num_layers, heads, device, forward_expansion, dropout: Parameters for configuring the encoder.\n",
        "    word_embedding: Embedding layer for words in the input sequence.\n",
        "    position_embedding: Positional encoding to provide information about the order of words.\n",
        "    layers: Stacked Transformer blocks.\n",
        "    dropout: Dropout layer for regularization."
      ],
      "metadata": {
        "id": "RG9HifS_SGaL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# The Transformer classes (MultiHeadAttention, TransformerBlock, and Encoder) go here...\n",
        "\n",
        "# 1. Create an instance of the Encoder class\n",
        "src_vocab_size = 1000  # Example vocabulary size\n",
        "embed_size = 256\n",
        "num_heads = 8\n",
        "num_layers = 4\n",
        "\n",
        "encoder = Encoder(src_vocab_size, embed_size, num_heads, num_layers)\n",
        "\n",
        "# 2. Generate a sample input sequence\n",
        "seq_length = 20\n",
        "sample_input = torch.randint(low=0, high=src_vocab_size, size=(1, seq_length))\n",
        "\n",
        "# 3. Define a mask (optional, for masking padded elements)\n",
        "mask = (sample_input != 0).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "# 4. Forward pass the input through the Transformer encoder\n",
        "output = encoder(sample_input, mask)\n",
        "\n",
        "# Print the output size\n",
        "print(\"Output Size:\", output.size())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwUYr1IaT8D7",
        "outputId": "8fdf8f6c-d4c2-44c8-d80b-af8348aa1f44"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output Size: torch.Size([1, 20, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "    Creates an instance of the Transformer model.\n",
        "    Generates a random input sequence.\n",
        "    Applies an attention mask to avoid processing padded elements.\n",
        "    Prints the size of the output tensor."
      ],
      "metadata": {
        "id": "pQdjJZCUSRHu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The size of the output tensor from the Transformer encoder block depends on the dimensions of your input sequence and the embedding size. In the example provided, the output size would be (batch_size, seq_length, embed_size).\n",
        "\n",
        "Let's break it down:\n",
        "\n",
        "    batch_size: The number of sequences you process in parallel (in this case, it's 1 as we generated a single sequence).\n",
        "    seq_length: The length of your input sequence (in this case, it's 20 as we generated a sequence of length 20).\n",
        "    embed_size: The size of the embedding vectors you defined for each token (in this case, it's 256).\n",
        "\n",
        "So, the expected output size would be (1, 20, 256)."
      ],
      "metadata": {
        "id": "x6B1Lp7XSZ2v"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UT8mPMXiUBSF"
      },
      "execution_count": 70,
      "outputs": []
    }
  ]
}